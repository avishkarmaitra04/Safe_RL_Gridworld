# Explainable Safe Reinforcement Learning for Hazard-Aware Navigation

This repository implements an **Explainable Safe Reinforcement Learning (Safe RL)** framework for hazard-aware navigation in a Gridworld environment using **Deep Q-Networks (DQN)**.  
The goal is to enable an agent to reach a target location while **minimizing safety violations** and maintaining **policy interpretability**.

---

## ğŸš€ Project Overview

Traditional Reinforcement Learning agents focus on reward maximization and often ignore safety constraints during exploration. This project addresses that limitation by integrating **safety-aware reward shaping** and **explainability techniques** into the learning process.

Key contributions:
- Safe navigation using penalty-based constraints
- Deep Q-Network (DQN) agent
- Policy visualization and state visitation heatmaps
- Trajectory analysis for explainability

---

## ğŸ§  Methodology

- **Environment**: Discrete Gridworld with safe cells, hazardous regions, start state, and goal state  
- **Agent**: Deep Q-Network (DQN)
- **Actions**: Up, Down, Left, Right
- **Safety Mechanism**: Strong negative penalties for hazardous states
- **Explainability**:
  - Agent trajectory visualization
  - Policy visualization
  - State visitation heatmaps

---

## ğŸ—ºï¸ Environment Details

- Grid size: `6 Ã— 6`
- Start state: Top-left corner
- Goal state: Bottom-right corner
- Hazardous cells: Penalized heavily and counted as safety violations

### Reward Structure
| Event | Reward |
|------|--------|
| Normal step | -1 |
| Hazardous state | -30 |
| Goal reached | +10 |

---

## âš™ï¸ Training Configuration

- Episodes: 500
- Max steps per episode: 50
- Discount factor (Î³): 0.99
- Learning rate: 0.001
- Exploration strategy: Îµ-greedy
- Replay buffer size: 10,000
- Batch size: 64

---

## ğŸ“Š Results

- **Success Rate**: 100%
- **Average Safety Violations**: 0.14
- **Improved cumulative reward over episodes**
- **Clear reduction in unsafe exploration**

Visual outputs include:
- Reward curve
- Safety violation plot
- Agent trajectory
- Policy visualization
- State visitation heatmap

---

## ğŸ“ Repository Structure

```text
Safe_RL_Gridworld/
â”‚
â”œâ”€â”€ environment.py        # Gridworld environment definition
â”œâ”€â”€ dqn_agent.py          # Deep Q-Network implementation
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ trajectory.py         # Agent trajectory visualization
â”œâ”€â”€ heatmap.py            # State visitation heatmap
â”œâ”€â”€ plots/                # Generated plots and figures
â”œâ”€â”€ models/               # Saved trained models
â”œâ”€â”€ README.md             # Project documentation
â””â”€â”€ report.pdf            # IEEE-style project report
